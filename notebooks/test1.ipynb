{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsarch example\n",
    "\n",
    "b-Bit Minwise Hashing\n",
    "\n",
    "http://research.microsoft.com/pubs/120078/wfc0398-lips.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan as Scan\n",
    "from elasticsearch.helpers import bulk as Bulk\n",
    "from elasticsearch_dsl import Search, Q\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from base64 import standard_b64decode as b64decode\n",
    "import BitVector\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import *\n",
    "import toyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'elasticsearch'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete old if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response: '{'acknowledged': True}'\n",
      " response: '{'acknowledged': True}'\n"
     ]
    }
   ],
   "source": [
    "if es.indices.exists(index='my_index'):\n",
    "    res = es.indices.delete(index='my_index')\n",
    "    print(\" response: '%s'\" % (res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and show new index with minhash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response: '{'acknowledged': True}'\n",
      " response: '{'my_index': {'mappings': {}, 'aliases': {}, 'settings': {'index': {'creation_date': '1446392764189', 'number_of_shards': '5', 'uuid': 'IZL-D8UFRr6SqCDy2X5zyg', 'analysis': {'analyzer': {'minhash_analyzer': {'tokenizer': 'standard', 'type': 'custom', 'filter': ['minhash']}}}, 'number_of_replicas': '1', 'version': {'created': '1070399'}}}, 'warmers': {}}}'\n",
      " response: '{'acknowledged': True}'\n",
      " response: '{'my_index': {'mappings': {}, 'aliases': {}, 'settings': {'index': {'creation_date': '1446392764189', 'number_of_shards': '5', 'uuid': 'IZL-D8UFRr6SqCDy2X5zyg', 'analysis': {'analyzer': {'minhash_analyzer': {'tokenizer': 'standard', 'type': 'custom', 'filter': ['minhash']}}}, 'number_of_replicas': '1', 'version': {'created': '1070399'}}}, 'warmers': {}}}'\n"
     ]
    }
   ],
   "source": [
    "my_index_settings_body = '''\n",
    "{\n",
    "  \"index\":{\n",
    "    \"analysis\":{\n",
    "      \"analyzer\":{\n",
    "        \"minhash_analyzer\":{\n",
    "          \"type\":\"custom\",\n",
    "          \"tokenizer\":\"standard\",\n",
    "          \"filter\":[\"minhash\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "res = es.indices.create(index='my_index', body=my_index_settings_body)\n",
    "print(\" response: '%s'\" % (res))\n",
    "res = es.indices.get(index='my_index')\n",
    "print(\" response: '%s'\" % (res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update and show mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response: '{'acknowledged': True}'\n",
      " response: '{'my_index': {'mappings': {'my_type': {'properties': {'minhash_value': {'minhash_analyzer': 'minhash_analyzer', 'copy_bits_to': 'bits', 'type': 'minhash'}, 'bits': {'store': True, 'type': 'string'}, 'message': {'copy_to': ['minhash_value'], 'type': 'string'}}}}}}'\n",
      " response: '{'acknowledged': True}'\n",
      " response: '{'my_index': {'mappings': {'my_type': {'properties': {'minhash_value': {'minhash_analyzer': 'minhash_analyzer', 'copy_bits_to': 'bits', 'type': 'minhash'}, 'bits': {'store': True, 'type': 'string'}, 'message': {'copy_to': ['minhash_value'], 'type': 'string'}}}}}}'\n"
     ]
    }
   ],
   "source": [
    "my_type_mapping_body = '''\n",
    "{\n",
    "  \"my_type\" : {\n",
    "    \"properties\" : {\n",
    "      \"message\" : {\n",
    "        \"type\" : \"string\",\n",
    "        \"copy_to\" : [\n",
    "          \"minhash_value\"\n",
    "        ]\n",
    "      },\n",
    "      \"bits\" : {\n",
    "        \"type\" : \"string\",\n",
    "        \"store\" : true\n",
    "      },\n",
    "      \"minhash_value\" : {\n",
    "        \"type\" : \"minhash\",\n",
    "        \"minhash_analyzer\" : \"minhash_analyzer\",\n",
    "        \"copy_bits_to\" : \"bits\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "res = es.indices.put_mapping(index='my_index', doc_type='my_type', body=my_type_mapping_body)\n",
    "print(\" response: '%s'\" % (res))\n",
    "res = es.indices.get_mapping(index='my_index', doc_type='my_type')\n",
    "print(\" response: '%s'\" % (res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch1(start, count):\n",
    "    k = {\n",
    "         '_index':'my_index',\n",
    "         '_type':'my_type',\n",
    "         '_id':0,\n",
    "         '_source':{}\n",
    "    }\n",
    "    for a in range(count):\n",
    "        k['_id'] = start + a\n",
    "        k['_source']['message'] = \"Prefix something %f suffix other\" % (random.random()*1000)\n",
    "        yield k\n",
    "\n",
    "def batch2(start, count):\n",
    "    k = {\n",
    "         '_index':'my_index',\n",
    "         '_type':'my_type',\n",
    "         '_id':0,\n",
    "         '_source':{}\n",
    "    }\n",
    "    for a in range(count):\n",
    "        k['_id'] = start + a\n",
    "        k['_source']['message'] = \"ERROR: Why we get this, now %d %f\" % (int(a/5), random.random()*1000)\n",
    "        yield k\n",
    "\n",
    "def batch3(start, count):\n",
    "    k = {\n",
    "         '_index':'my_index',\n",
    "         '_type':'my_type',\n",
    "         '_id':0,\n",
    "         '_source':{}\n",
    "    }\n",
    "    for a in range(count):\n",
    "        k['_id'] = start + a\n",
    "        k['_source']['message'] = 'DEBUG: This message is entirely not for any use in any way.'\n",
    "        yield k\n",
    "\n",
    "Bulk(es, batch1(count=10000, start=0))\n",
    "Bulk(es, batch2(count=10000, start=10000))\n",
    "Bulk(es, batch3(count=10000, start=20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct search using dsl, and scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Search(using=es, index=\"my_index\") \\\n",
    ".query(\"term\", _type=\"my_type\") \\\n",
    ".fields(['message', 'minhash_value'])\n",
    "#print(s.to_dict())\n",
    "response = Scan(es, query=s.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "maxid = 30000\n",
    "cm = int((maxid-1)/10000)+1\n",
    "bm = 2\n",
    "bfreq = np.zeros([maxlen, bm])\n",
    "cnt = np.zeros([maxlen, bm, cm])\n",
    "resp_limit = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in response:\n",
    "    if resp_limit is not None:\n",
    "        if resp_limit < 0:\n",
    "            break\n",
    "        else:\n",
    "            resp_limit -= 1\n",
    "    #print(doc)\n",
    "    id = int(doc['_id'])\n",
    "    t = int((id-1)/10000)\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        bits = BitVector.BitVector(rawbytes = b64decode(mhv))\n",
    "        i = 0\n",
    "        for b in bits:\n",
    "            bfreq[i,b] += 1\n",
    "            cnt[i,b,t] += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe print result fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(maxlen):\n",
    "        for b in range(bm):\n",
    "            v = bfreq[i,b]\n",
    "            print(\"%d %d %d\" % (i, b, v))\n",
    "            if v == 0:\n",
    "                continue\n",
    "            for t in range(cm):\n",
    "                c = cnt[i,b,t]\n",
    "                print(\"  %d %d %f\" % (t, c, c / v))\n",
    "\n",
    "print(bfreq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print binary distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "x = np.transpose(np.arange(maxlen))\n",
    "y0 = bfreq[x,0]\n",
    "y1 = bfreq[x,1]\n",
    "p = figure()\n",
    "p.circle(x, y0, legend=\"0\")\n",
    "p.circle(x, y1, legend=\"1\", fill_color=\"red\")\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = np.transpose(np.arange(maxlen))\n",
    "x = bfreq[r,0]\n",
    "y0 = cnt[r,0,0]/bfreq[r,0]\n",
    "y1 = cnt[r,0,1]/bfreq[r,0]\n",
    "y2 = cnt[r,0,2]/bfreq[r,0]\n",
    "p = figure()\n",
    "p.circle(x, y0, legend=\"c0-0\")\n",
    "p.circle(x, y1, legend=\"c0-1\", fill_color=\"red\")\n",
    "p.circle(x, y2, legend=\"c0-2\", fill_color=\"cyan\")\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try toyplot too, somehow x is not what it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canvas = toyplot.Canvas(width=640, height=480)\n",
    "axes = canvas.axes()\n",
    "x = bfreq[r,0]\n",
    "mark0 = axes.scatterplot(x,y0)\n",
    "mark1 = axes.scatterplot(x,y1)\n",
    "mark2 = axes.scatterplot(x,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try AdaBoost\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html\n",
    "First train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_estimators = 400\n",
    "train_cut = 2000\n",
    "\n",
    "ada_real = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "\n",
    "xm = train_cut*cm\n",
    "X_train = np.zeros([xm, maxlen], dtype=np.byte)\n",
    "y_train = np.zeros([xm])\n",
    "\n",
    "response = Scan(es, query=s.to_dict())\n",
    "for doc in response:\n",
    "    id = int(doc['_id'])\n",
    "    if (id%10000) >= train_cut:\n",
    "        continue\n",
    "    t = int(id/10000)\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        bits = BitVector.BitVector(rawbytes = b64decode(mhv))\n",
    "        X_train[id%10000+train_cut*t] = bits\n",
    "        y_train[id%10000+train_cut*t] = t\n",
    "        \n",
    "ada_real.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_real_err = np.zeros((n_estimators,))\n",
    "\n",
    "xm = maxid - train_cut*cm\n",
    "X_test = np.zeros([xm, maxlen], dtype=np.byte)\n",
    "y_test = np.zeros([xm])\n",
    "\n",
    "response = Scan(es, query=s.to_dict())\n",
    "for doc in response:\n",
    "    id = int(doc['_id'])\n",
    "    if (id%10000) < train_cut:\n",
    "        continue\n",
    "    t = int(id/10000)\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        bits = BitVector.BitVector(rawbytes = b64decode(mhv))\n",
    "        X_test[id-train_cut*(t+1)] = bits\n",
    "        y_test[id-train_cut*(t+1)] = t\n",
    "\n",
    "for i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n",
    "    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n",
    "\n",
    "print(ada_real_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test result, seems prediction is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = figure()\n",
    "pred = ada_real.predict(X_test)\n",
    "x=np.arange(pred.shape[0])\n",
    "p.circle(x,pred)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use AdaBoost for data as 64-bit vlues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_real2 = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "\n",
    "xm = train_cut*cm\n",
    "xlen = int(maxlen/64)\n",
    "X_train = np.zeros([xm,xlen], dtype=np.uint64)\n",
    "y_train = np.zeros([xm])\n",
    "x_a = np.zeros([xlen], dtype=np.uint64)\n",
    "\n",
    "response = Scan(es, query=s.to_dict())\n",
    "for doc in response:\n",
    "    id = int(doc['_id'])\n",
    "    if (id%10000) >= train_cut:\n",
    "        continue\n",
    "    t = int(id/10000)\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        X_train[id%10000+train_cut*t] = struct.unpack('QQ', b64decode(mhv))\n",
    "        y_train[id%10000+train_cut*t] = t\n",
    "        \n",
    "ada_real2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_real_err2 = np.zeros((n_estimators,))\n",
    "\n",
    "xm = maxid - train_cut*cm\n",
    "X_test = np.zeros([xm,xlen])\n",
    "y_test = np.zeros([xm])\n",
    "\n",
    "response = Scan(es, query=s.to_dict())\n",
    "for doc in response:\n",
    "    id = int(doc['_id'])\n",
    "    if (id%10000) < train_cut:\n",
    "        continue\n",
    "    t = int(id/10000)\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        X_test[id-train_cut*(t+1)] = struct.unpack('QQ', b64decode(mhv))\n",
    "        y_test[id-train_cut*(t+1)] = t\n",
    "\n",
    "for i, y_pred in enumerate(ada_real2.staged_predict(X_test)):\n",
    "    ada_real_err2[i] = zero_one_loss(y_pred, y_test)\n",
    "\n",
    "print(ada_real_err2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems not to be working\n",
    "\n",
    "Try something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_full = np.zeros([maxid, maxlen], dtype=np.byte)\n",
    "X_popc = np.zeros([maxlen], dtype=np.uint32)\n",
    "\n",
    "response = Scan(es, query=s.to_dict())\n",
    "for doc in response:\n",
    "    id = int(doc['_id'])\n",
    "    for mhv in doc['fields']['minhash_value']:\n",
    "        rawbytes = b64decode(mhv)\n",
    "        bits = BitVector.BitVector(rawbytes = rawbytes)\n",
    "        parts = struct.unpack('QQ', rawbytes)\n",
    "        popc = gmpy2.popcount(gmpy2.mpz(parts[0])) + gmpy2.popcount(gmpy2.mpz(parts[1]))\n",
    "        X_full[id] = bits\n",
    "        X_popc[popc] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_full[0])\n",
    "print(X_popc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup = \"\"\"\n",
    "import numpy as np\n",
    "import gmpy2\n",
    "from gmpy2 import mpz\n",
    "\n",
    "POPCOUNT_TABLE16 = np.zeros(2**16, dtype=int) #has to be an array\n",
    "POPCOUNT_TABLE16b = np.zeros(2**16, dtype=np.byte) #has to be an array\n",
    "\n",
    "for index in range(len(POPCOUNT_TABLE16)):\n",
    "    POPCOUNT_TABLE16[index] = (index & 1) + POPCOUNT_TABLE16[index >> 1]\n",
    "    POPCOUNT_TABLE16b[index] = (index & 1) + POPCOUNT_TABLE16b[index >> 1]\n",
    "\n",
    "def popcount32_table16(v):\n",
    "    return (POPCOUNT_TABLE16[ v        & 0xffff] +\n",
    "            POPCOUNT_TABLE16[(v >> 16) & 0xffff])\n",
    "\n",
    "def popcount32_table16b(v):\n",
    "    return (POPCOUNT_TABLE16b[ v        & 0xffff] +\n",
    "            POPCOUNT_TABLE16b[(v >> 16) & 0xffff])\n",
    "\n",
    "def count1s(v):\n",
    "    return popcount32_table16(v).sum()\n",
    "\n",
    "def count1sb(v):\n",
    "    return popcount32_table16b(v).sum()\n",
    "\n",
    "def gmpy2count1s(v):\n",
    "    return sum(gmpy2.popcount(mpz(int(a))) for a in v)\n",
    "\n",
    "v1 = np.arange(1000)*1234567                       #numpy array\n",
    "\"\"\"\n",
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(timeit(\"count1s(v1)\", setup=setup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(timeit(\"count1sb(v1)\", setup=setup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(timeit(\"gmpy2count1s(v1)\", setup=setup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_full_std = StandardScaler().fit_transform(X_full)\n",
    "db = DBSCAN(eps=0.3, min_samples=3).fit(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
